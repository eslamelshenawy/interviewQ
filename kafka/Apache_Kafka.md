# Ø¯Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù€ Apache Kafka

## Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª
1. [Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Kafka](#Ù…Ù‚Ø¯Ù…Ø©-Ø¹Ù†-kafka)
2. [Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©](#Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…-Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©)
3. [Ø¨Ù†ÙŠØ© Kafka Architecture](#Ø¨Ù†ÙŠØ©-kafka-architecture)
4. [Ø§Ù„ØªØ«Ø¨ÙŠØª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯](#Ø§Ù„ØªØ«Ø¨ÙŠØª-ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯)
5. [Producer API](#producer-api)
6. [Consumer API](#consumer-api)
7. [Kafka Streams](#kafka-streams)
8. [Kafka Connect](#kafka-connect)
9. [Ø¥Ø¯Ø§Ø±Ø© Topics](#Ø¥Ø¯Ø§Ø±Ø©-topics)
10. [Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©](#Ø£Ù…Ø«Ù„Ø©-Ø¹Ù…Ù„ÙŠØ©)

---

## Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Kafka

### Ù…Ø§ Ù‡Ùˆ Apache KafkaØŸ
**Apache Kafka** Ù‡Ùˆ Ù†Ø¸Ø§Ù… **distributed streaming platform** ÙŠÙØ³ØªØ®Ø¯Ù… Ù„Ø¨Ù†Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚Ø§Øª real-time data pipelines Ùˆ streaming applications.

### Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

```
ğŸ“Š Messaging System       - Ù†Ø¸Ø§Ù… Ø±Ø³Ø§Ø¦Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø£Ù†Ø¸Ù…Ø©
ğŸ“ˆ Activity Tracking      - ØªØªØ¨Ø¹ Ù†Ø´Ø§Ø· Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
ğŸ“ Log Aggregation        - ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
ğŸ”„ Stream Processing      - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¯ÙÙ‚Ø©
ğŸ’¾ Event Sourcing         - Ø­ÙØ¸ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
ğŸ“¡ Commit Log             - Ø³Ø¬Ù„ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª
```

### Ù…Ù…ÙŠØ²Ø§Øª Kafka

âœ… **High Throughput** - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ø§ÙŠÙŠÙ† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©
âœ… **Scalability** - Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹ Ø£ÙÙ‚ÙŠØ§Ù‹
âœ… **Durability** - Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¯Ø§Ø¦Ù…
âœ… **Fault Tolerance** - ØªØ­Ù…Ù„ Ø§Ù„Ø£Ø¹Ø·Ø§Ù„
âœ… **Low Latency** - Ø²Ù…Ù† Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù‚Ù„ÙŠÙ„ (milliseconds)
âœ… **Distributed** - Ù…ÙˆØ²Ø¹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© servers

---

## Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

### 1. Topic (Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹)

```
Topic = ÙØ¦Ø© Ø£Ùˆ ØªØµÙ†ÙŠÙ Ù„Ù„Ø±Ø³Ø§Ø¦Ù„

Ù…Ø«Ø§Ù„:
- orders           (Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø´Ø±Ø§Ø¡)
- user-clicks      (Ù†Ù‚Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†)
- system-logs      (Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…)
- payments         (Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª)
```

### 2. Partition (Ø§Ù„ØªÙ‚Ø³ÙŠÙ…)

```
ÙƒÙ„ Topic Ù…Ù‚Ø³Ù… Ø¥Ù„Ù‰ Partitions

Topic: orders
â”œâ”€â”€ Partition 0: [msg1, msg2, msg3]
â”œâ”€â”€ Partition 1: [msg4, msg5, msg6]
â””â”€â”€ Partition 2: [msg7, msg8, msg9]

Ø§Ù„ÙÙˆØ§Ø¦Ø¯:
âœ… Parallelism - Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ©
âœ… Scalability - Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹
âœ… Ordering - ØªØ±ØªÙŠØ¨ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¯Ø§Ø®Ù„ Partition
```

### 3. Offset (Ø§Ù„Ø¥Ø²Ø§Ø­Ø©)

```
Ø±Ù‚Ù… ØªØ³Ù„Ø³Ù„ÙŠ ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ Ø±Ø³Ø§Ù„Ø© Ø¯Ø§Ø®Ù„ Partition

Partition 0:
Offset 0: "Order #1"
Offset 1: "Order #2"
Offset 2: "Order #3"
Offset 3: "Order #4"
       â†‘
   Ø§Ù„Ù…Ø³ØªÙ‡Ù„Ùƒ Ù‡Ù†Ø§ (ÙŠÙ‚Ø±Ø£ Ù…Ù† offset 3)
```

### 4. Producer (Ø§Ù„Ù…ÙÙ†ØªØ¬)

```java
// ÙŠØ±Ø³Ù„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¥Ù„Ù‰ Topics
Producer â†’ Topic â†’ Partitions
```

### 5. Consumer (Ø§Ù„Ù…ÙØ³ØªÙ‡Ù„Ùƒ)

```java
// ÙŠÙ‚Ø±Ø£ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù…Ù† Topics
Consumer â† Topic â† Partitions
```

### 6. Consumer Group

```
Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Consumers ÙŠØ¹Ù…Ù„ÙˆÙ† Ù…Ø¹Ø§Ù‹

Topic: orders (3 partitions)
â”œâ”€â”€ Consumer Group: "order-processors"
    â”œâ”€â”€ Consumer 1 â†’ reads Partition 0
    â”œâ”€â”€ Consumer 2 â†’ reads Partition 1
    â””â”€â”€ Consumer 3 â†’ reads Partition 2

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯:
âœ… ÙƒÙ„ partition ÙŠÙÙ‚Ø±Ø£ Ø¨ÙˆØ§Ø³Ø·Ø© consumer ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù€ group
âœ… Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹Ø¯Ø¯ consumers Ø£ÙƒØ¨Ø± Ù…Ù† partitionsØŒ Ø¨Ø¹Ø¶Ù‡Ù… Ø³ÙŠÙƒÙˆÙ† idle
âœ… Ø¥Ø°Ø§ ÙØ´Ù„ consumerØŒ partitions ØªÙˆØ²Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø§Ù‚ÙŠÙŠÙ†
```

### 7. Broker

```
Kafka Server - Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø°ÙŠ ÙŠØ®Ø²Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

Kafka Cluster:
â”œâ”€â”€ Broker 1 (Leader for Partition 0, 2)
â”œâ”€â”€ Broker 2 (Leader for Partition 1)
â””â”€â”€ Broker 3 (Replicas)
```

### 8. Replication (Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªÙ…Ø§Ø«Ù„)

```
Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª

Topic: orders (Replication Factor = 3)

Partition 0:
â”œâ”€â”€ Leader: Broker 1
â”œâ”€â”€ Replica 1: Broker 2
â””â”€â”€ Replica 2: Broker 3

Ø¥Ø°Ø§ ÙØ´Ù„ Broker 1:
â†’ Broker 2 ÙŠØµØ¨Ø­ Leader Ø¬Ø¯ÙŠØ¯
```

---

## Ø¨Ù†ÙŠØ© Kafka Architecture

### Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¹Ø§Ù…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Kafka Cluster                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Broker 1â”‚  â”‚ Broker 2â”‚  â”‚ Broker 3â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â†‘            â†‘            â†‘           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚            â”‚            â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚           â”‚             â”‚        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”
â”‚Producerâ”‚ â”‚Producerâ”‚   â”‚Consumerâ”‚ â”‚Consumerâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
```

### ZooKeeper (Ù‚Ø¯ÙŠÙ…) vs KRaft (Ø¬Ø¯ÙŠØ¯)

```
# ZooKeeper Mode (Ù‚Ø¨Ù„ Kafka 2.8)
ZooKeeper
    â†“
ÙŠØ¯ÙŠØ± Metadata Ù„Ù„Ù€ Brokers

# KRaft Mode (Kafka 3.0+)
Ø¨Ø¯ÙˆÙ† ZooKeeper
Kafka ÙŠØ¯ÙŠØ± Ù†ÙØ³Ù‡
```

---

## Ø§Ù„ØªØ«Ø¨ÙŠØª ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯

### Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¹Ù„Ù‰ Windows

```bash
# 1. ØªØ­Ù…ÙŠÙ„ Kafka
# Ù…Ù† https://kafka.apache.org/downloads

# 2. ÙÙƒ Ø§Ù„Ø¶ØºØ·
cd C:\kafka

# 3. ØªØ´ØºÙŠÙ„ ZooKeeper (Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… ZooKeeper mode)
bin\windows\zookeeper-server-start.bat config\zookeeper.properties

# 4. ØªØ´ØºÙŠÙ„ Kafka Broker
bin\windows\kafka-server-start.bat config\server.properties
```

### Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¹Ù„Ù‰ Linux/Mac

```bash
# 1. ØªØ­Ù…ÙŠÙ„ ÙˆÙÙƒ Ø§Ù„Ø¶ØºØ·
wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
tar -xzf kafka_2.13-3.6.0.tgz
cd kafka_2.13-3.6.0

# 2. ØªØ´ØºÙŠÙ„ ZooKeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# 3. ØªØ´ØºÙŠÙ„ Kafka
bin/kafka-server-start.sh config/server.properties
```

### Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Docker

```yaml
# docker-compose.yml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
```

```bash
# ØªØ´ØºÙŠÙ„
docker-compose up -d

# Ø¥ÙŠÙ‚Ø§Ù
docker-compose down
```

---

## Producer API

### Maven Dependencies

```xml
<!-- pom.xml -->
<dependencies>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>3.6.0</version>
    </dependency>

    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-simple</artifactId>
        <version>2.0.9</version>
    </dependency>
</dependencies>
```

### Producer Ø¨Ø³ÙŠØ·

```java
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        // 1. Ø¥Ø¹Ø¯Ø§Ø¯ Properties
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
            StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
            StringSerializer.class.getName());

        // 2. Ø¥Ù†Ø´Ø§Ø¡ Producer
        KafkaProducer<String, String> producer =
            new KafkaProducer<>(props);

        // 3. Ø¥Ù†Ø´Ø§Ø¡ Record
        ProducerRecord<String, String> record =
            new ProducerRecord<>("my-topic", "key1", "Hello Kafka!");

        // 4. Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø©
        try {
            producer.send(record);
            System.out.println("Message sent successfully");
        } finally {
            producer.close();
        }
    }
}
```

### Producer Ù…Ø¹ Callback

```java
public class ProducerWithCallback {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", StringSerializer.class.getName());

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        for (int i = 0; i < 10; i++) {
            ProducerRecord<String, String> record =
                new ProducerRecord<>("my-topic", "key-" + i, "message-" + i);

            // Ø¥Ø±Ø³Ø§Ù„ Ù…Ø¹ Callback
            producer.send(record, new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception exception) {
                    if (exception == null) {
                        System.out.println("Message sent successfully:");
                        System.out.println("Topic: " + metadata.topic());
                        System.out.println("Partition: " + metadata.partition());
                        System.out.println("Offset: " + metadata.offset());
                        System.out.println("Timestamp: " + metadata.timestamp());
                    } else {
                        System.err.println("Error sending message: " +
                            exception.getMessage());
                    }
                }
            });
        }

        producer.close();
    }
}
```

### Producer Ù…Ø¹ Custom Partitioner

```java
// Custom Partitioner
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;

public class CustomPartitioner implements Partitioner {
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
                        Object value, byte[] valueBytes, Cluster cluster) {
        // Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "VIP" ØªØ°Ù‡Ø¨ Ù„Ù„Ù€ Partition 0
        if (value.toString().contains("VIP")) {
            return 0;
        }

        // Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ ØªÙˆØ²Ø¹ Ø¨Ø´ÙƒÙ„ round-robin
        int numPartitions = cluster.partitionCountForTopic(topic);
        return Math.abs(key.hashCode()) % (numPartitions - 1) + 1;
    }

    @Override
    public void close() {}

    @Override
    public void configure(Map<String, ?> configs) {}
}

// Ø§Ø³ØªØ®Ø¯Ø§Ù… Custom Partitioner
public class ProducerWithCustomPartitioner {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", StringSerializer.class.getName());
        props.put("partitioner.class", CustomPartitioner.class.getName());

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        producer.send(new ProducerRecord<>("my-topic", "user1", "VIP Customer"));
        producer.send(new ProducerRecord<>("my-topic", "user2", "Regular Customer"));

        producer.close();
    }
}
```

### Producer Ù…Ø¹ JSON

```java
// POJO Class
class Order {
    private String orderId;
    private String customerId;
    private double amount;

    // Constructor, Getters, Setters
    public Order(String orderId, String customerId, double amount) {
        this.orderId = orderId;
        this.customerId = customerId;
        this.amount = amount;
    }

    public String getOrderId() { return orderId; }
    public String getCustomerId() { return customerId; }
    public double getAmount() { return amount; }
}

// JSON Serializer
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.kafka.common.serialization.Serializer;

public class JsonSerializer<T> implements Serializer<T> {
    private final ObjectMapper objectMapper = new ObjectMapper();

    @Override
    public byte[] serialize(String topic, T data) {
        try {
            return objectMapper.writeValueAsBytes(data);
        } catch (Exception e) {
            throw new RuntimeException("Error serializing JSON", e);
        }
    }
}

// Producer
public class JsonProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", JsonSerializer.class.getName());

        KafkaProducer<String, Order> producer = new KafkaProducer<>(props);

        Order order = new Order("ORD-001", "CUST-123", 1500.50);
        ProducerRecord<String, Order> record =
            new ProducerRecord<>("orders", order.getOrderId(), order);

        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                System.out.println("Order sent: " + order.getOrderId());
            } else {
                exception.printStackTrace();
            }
        });

        producer.close();
    }
}
```

### Producer Configurations Ø§Ù„Ù…Ù‡Ù…Ø©

```java
Properties props = new Properties();

// âœ… Required
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", StringSerializer.class.getName());
props.put("value.serializer", StringSerializer.class.getName());

// âš™ï¸ Performance
props.put("batch.size", 16384);           // Ø­Ø¬Ù… Ø§Ù„Ù€ batch
props.put("linger.ms", 10);               // Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ø¨Ù„ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„
props.put("buffer.memory", 33554432);     // Ø­Ø¬Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ù€ buffer
props.put("compression.type", "snappy");  // Ø§Ù„Ø¶ØºØ· (snappy, gzip, lz4, zstd)

// ğŸ”’ Reliability
props.put("acks", "all");                 // 0, 1, all/-1
props.put("retries", 3);                  // Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
props.put("max.in.flight.requests.per.connection", 5);
props.put("enable.idempotence", true);    // Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±

// ğŸ”‘ Partitioning
props.put("partitioner.class", "org.apache.kafka.clients.producer.RoundRobinPartitioner");
```

### Acknowledgment Modes (acks)

```java
// acks = 0 (Ø£Ø³Ø±Ø¹ØŒ ØºÙŠØ± Ø¢Ù…Ù†)
// Producer Ù„Ø§ ÙŠÙ†ØªØ¸Ø± ØªØ£ÙƒÙŠØ¯ Ù…Ù† Broker
props.put("acks", "0");

// acks = 1 (Ù…ØªÙˆØ³Ø·)
// Producer ÙŠÙ†ØªØ¸Ø± ØªØ£ÙƒÙŠØ¯ Ù…Ù† Leader ÙÙ‚Ø·
props.put("acks", "1");

// acks = all/-1 (Ø£Ø¨Ø·Ø£ØŒ Ø£ÙƒØ«Ø± Ø£Ù…Ø§Ù†Ø§Ù‹)
// Producer ÙŠÙ†ØªØ¸Ø± ØªØ£ÙƒÙŠØ¯ Ù…Ù† Leader ÙˆØ¬Ù…ÙŠØ¹ Replicas
props.put("acks", "all");
props.put("min.insync.replicas", 2);  // ÙÙŠ server.properties
```

---

## Consumer API

### Consumer Ø¨Ø³ÙŠØ·

```java
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import java.time.Duration;
import java.util.*;

public class SimpleConsumer {
    public static void main(String[] args) {
        // 1. Ø¥Ø¹Ø¯Ø§Ø¯ Properties
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "my-consumer-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
            StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
            StringDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");

        // 2. Ø¥Ù†Ø´Ø§Ø¡ Consumer
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        // 3. Subscribe to Topic
        consumer.subscribe(Collections.singletonList("my-topic"));

        // 4. Poll for messages
        try {
            while (true) {
                ConsumerRecords<String, String> records =
                    consumer.poll(Duration.ofMillis(100));

                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Topic: %s, Partition: %d, Offset: %d, " +
                        "Key: %s, Value: %s%n",
                        record.topic(), record.partition(), record.offset(),
                        record.key(), record.value());
                }
            }
        } finally {
            consumer.close();
        }
    }
}
```

### Consumer Ù…Ø¹ Manual Commit

```java
public class ManualCommitConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "my-group");
        props.put("key.deserializer", StringDeserializer.class.getName());
        props.put("value.deserializer", StringDeserializer.class.getName());
        props.put("enable.auto.commit", "false");  // Manual commit

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Arrays.asList("my-topic"));

        try {
            while (true) {
                ConsumerRecords<String, String> records =
                    consumer.poll(Duration.ofMillis(100));

                for (ConsumerRecord<String, String> record : records) {
                    System.out.println("Processing: " + record.value());

                    // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø©...
                    processRecord(record);
                }

                // Commit Ø¨Ø¹Ø¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
                consumer.commitSync();
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            consumer.close();
        }
    }

    private static void processRecord(ConsumerRecord<String, String> record) {
        // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø©
        System.out.println("Processed: " + record.value());
    }
}
```

### Consumer Ù…Ø¹ Async Commit

```java
public class AsyncCommitConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "async-group");
        props.put("key.deserializer", StringDeserializer.class.getName());
        props.put("value.deserializer", StringDeserializer.class.getName());
        props.put("enable.auto.commit", "false");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Arrays.asList("my-topic"));

        try {
            while (true) {
                ConsumerRecords<String, String> records =
                    consumer.poll(Duration.ofMillis(100));

                for (ConsumerRecord<String, String> record : records) {
                    System.out.println("Value: " + record.value());
                }

                // Async commit Ù…Ø¹ callback
                consumer.commitAsync(new OffsetCommitCallback() {
                    @Override
                    public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets,
                                          Exception exception) {
                        if (exception != null) {
                            System.err.println("Commit failed: " +
                                exception.getMessage());
                        } else {
                            System.out.println("Commit successful");
                        }
                    }
                });
            }
        } finally {
            // Sync commit Ø¹Ù†Ø¯ Ø§Ù„Ø¥ØºÙ„Ø§Ù‚
            try {
                consumer.commitSync();
            } finally {
                consumer.close();
            }
        }
    }
}
```

### Consumer Ù…Ø¹ Specific Partition

```java
public class PartitionConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "partition-group");
        props.put("key.deserializer", StringDeserializer.class.getName());
        props.put("value.deserializer", StringDeserializer.class.getName());

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        // Assign specific partitions
        TopicPartition partition0 = new TopicPartition("my-topic", 0);
        TopicPartition partition1 = new TopicPartition("my-topic", 1);
        consumer.assign(Arrays.asList(partition0, partition1));

        // Seek to specific offset
        consumer.seek(partition0, 10);  // Ø§Ø¨Ø¯Ø£ Ù…Ù† offset 10

        while (true) {
            ConsumerRecords<String, String> records =
                consumer.poll(Duration.ofMillis(100));

            for (ConsumerRecord<String, String> record : records) {
                System.out.printf("Partition: %d, Offset: %d, Value: %s%n",
                    record.partition(), record.offset(), record.value());
            }
        }
    }
}
```

### Consumer Rebalancing

```java
import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
import org.apache.kafka.common.TopicPartition;

public class RebalanceConsumer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "rebalance-group");
        props.put("key.deserializer", StringDeserializer.class.getName());
        props.put("value.deserializer", StringDeserializer.class.getName());
        props.put("enable.auto.commit", "false");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        // Subscribe Ù…Ø¹ RebalanceListener
        consumer.subscribe(Arrays.asList("my-topic"),
            new ConsumerRebalanceListener() {
                @Override
                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
                    System.out.println("Partitions revoked: " + partitions);
                    // Commit offsets Ù‚Ø¨Ù„ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ù€ partitions
                    consumer.commitSync();
                }

                @Override
                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
                    System.out.println("Partitions assigned: " + partitions);
                }
            });

        try {
            while (true) {
                ConsumerRecords<String, String> records =
                    consumer.poll(Duration.ofMillis(100));

                for (ConsumerRecord<String, String> record : records) {
                    System.out.println(record.value());
                }

                consumer.commitAsync();
            }
        } finally {
            consumer.close();
        }
    }
}
```

### Consumer Configurations Ø§Ù„Ù…Ù‡Ù…Ø©

```java
Properties props = new Properties();

// âœ… Required
props.put("bootstrap.servers", "localhost:9092");
props.put("group.id", "my-consumer-group");
props.put("key.deserializer", StringDeserializer.class.getName());
props.put("value.deserializer", StringDeserializer.class.getName());

// ğŸ“– Offset Management
props.put("auto.offset.reset", "earliest");  // earliest, latest, none
props.put("enable.auto.commit", "true");
props.put("auto.commit.interval.ms", "5000");

// âš™ï¸ Performance
props.put("fetch.min.bytes", "1");           // Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
props.put("fetch.max.wait.ms", "500");       // Ø£Ù‚ØµÙ‰ ÙˆÙ‚Øª Ø§Ù†ØªØ¸Ø§Ø±
props.put("max.partition.fetch.bytes", "1048576");  // 1MB
props.put("max.poll.records", "500");        // Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù„ÙƒÙ„ poll

// ğŸ”„ Rebalancing
props.put("session.timeout.ms", "10000");
props.put("heartbeat.interval.ms", "3000");
props.put("max.poll.interval.ms", "300000");
```

---

## Kafka Streams

### Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ· - Word Count

```java
import org.apache.kafka.streams.*;
import org.apache.kafka.streams.kstream.*;
import java.util.Properties;
import java.util.Arrays;

public class WordCountStream {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "word-count-app");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,
            Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,
            Serdes.String().getClass());

        StreamsBuilder builder = new StreamsBuilder();

        // 1. Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† topic
        KStream<String, String> textLines =
            builder.stream("input-topic");

        // 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        KTable<String, Long> wordCounts = textLines
            // ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
            .flatMapValues(value -> Arrays.asList(value.toLowerCase().split("\\W+")))
            // ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø©
            .groupBy((key, word) -> word)
            // Ø¹Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            .count();

        // 3. ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
        wordCounts.toStream()
            .to("output-topic", Produced.with(Serdes.String(), Serdes.Long()));

        // 4. ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();

        // Shutdown hook
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

### Stream Transformations

```java
public class StreamTransformations {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "transformations-app");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");

        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> source = builder.stream("input-topic");

        // 1. filter
        KStream<String, String> filtered = source
            .filter((key, value) -> value.length() > 10);

        // 2. map
        KStream<String, String> mapped = source
            .map((key, value) -> KeyValue.pair(key.toUpperCase(), value.toUpperCase()));

        // 3. flatMap
        KStream<String, String> flatMapped = source
            .flatMap((key, value) -> {
                List<KeyValue<String, String>> result = new ArrayList<>();
                for (String word : value.split(" ")) {
                    result.add(KeyValue.pair(key, word));
                }
                return result;
            });

        // 4. branch - ØªÙ‚Ø³ÙŠÙ… Stream
        KStream<String, String>[] branches = source.branch(
            (key, value) -> value.contains("ERROR"),
            (key, value) -> value.contains("WARNING"),
            (key, value) -> true  // Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
        );

        KStream<String, String> errors = branches[0];
        KStream<String, String> warnings = branches[1];
        KStream<String, String> others = branches[2];

        // 5. merge - Ø¯Ù…Ø¬ Streams
        KStream<String, String> merged = errors.merge(warnings);

        // 6. peek - Ù„Ù„Ù€ debugging
        source.peek((key, value) ->
            System.out.println("Key: " + key + ", Value: " + value));
    }
}
```

### Joining Streams

```java
public class StreamJoins {
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();

        // Stream 1: Orders
        KStream<String, String> orders = builder.stream("orders");

        // Stream 2: Payments
        KStream<String, String> payments = builder.stream("payments");

        // Join Window - 5 Ø¯Ù‚Ø§Ø¦Ù‚
        JoinWindows joinWindow = JoinWindows.of(Duration.ofMinutes(5));

        // Inner Join
        KStream<String, String> joined = orders.join(
            payments,
            (orderValue, paymentValue) ->
                "Order: " + orderValue + ", Payment: " + paymentValue,
            joinWindow
        );

        // Left Join
        KStream<String, String> leftJoined = orders.leftJoin(
            payments,
            (orderValue, paymentValue) ->
                "Order: " + orderValue +
                ", Payment: " + (paymentValue != null ? paymentValue : "PENDING"),
            joinWindow
        );

        // Outer Join
        KStream<String, String> outerJoined = orders.outerJoin(
            payments,
            (orderValue, paymentValue) ->
                "Order: " + (orderValue != null ? orderValue : "NONE") +
                ", Payment: " + (paymentValue != null ? paymentValue : "NONE"),
            joinWindow
        );

        joined.to("joined-topic");
    }
}
```

### Windowing

```java
public class WindowingExample {
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, Long> clicks = builder.stream("user-clicks");

        // 1. Tumbling Window (Ù†Ø§ÙØ°Ø© Ø«Ø§Ø¨ØªØ©)
        KTable<Windowed<String>, Long> tumblingCounts = clicks
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count();

        // 2. Hopping Window (Ù†Ø§ÙØ°Ø© Ù…ØªØ¯Ø§Ø®Ù„Ø©)
        KTable<Windowed<String>, Long> hoppingCounts = clicks
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5))
                .advanceBy(Duration.ofMinutes(1)))
            .count();

        // 3. Session Window (Ù†Ø§ÙØ°Ø© Ø¬Ù„Ø³Ø©)
        KTable<Windowed<String>, Long> sessionCounts = clicks
            .groupByKey()
            .windowedBy(SessionWindows.with(Duration.ofMinutes(10)))
            .count();

        // ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
        tumblingCounts.toStream()
            .map((key, value) -> KeyValue.pair(
                key.key() + "@" + key.window().start() + "->" + key.window().end(),
                value
            ))
            .to("windowed-counts");
    }
}
```

---

## Kafka Connect

### Source Connector - Ù‚Ø±Ø§Ø¡Ø© Ù…Ù† Database

```json
{
  "name": "mysql-source-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "tasks.max": "1",
    "connection.url": "jdbc:mysql://localhost:3306/mydb",
    "connection.user": "user",
    "connection.password": "password",
    "table.whitelist": "orders,customers",
    "mode": "incrementing",
    "incrementing.column.name": "id",
    "topic.prefix": "mysql-"
  }
}
```

### Sink Connector - ÙƒØªØ§Ø¨Ø© Ø¥Ù„Ù‰ Elasticsearch

```json
{
  "name": "elasticsearch-sink-connector",
  "config": {
    "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
    "tasks.max": "1",
    "topics": "orders,customers",
    "connection.url": "http://localhost:9200",
    "type.name": "_doc",
    "key.ignore": "true",
    "schema.ignore": "true"
  }
}
```

### Ø¥Ø¯Ø§Ø±Ø© Connectors

```bash
# Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Connectors
curl http://localhost:8083/connectors

# Ø¥Ù†Ø´Ø§Ø¡ Connector
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @connector-config.json

# Ø­Ø°Ù Connector
curl -X DELETE http://localhost:8083/connectors/mysql-source-connector

# Ø¥ÙŠÙ‚Ø§Ù Ù…Ø¤Ù‚Øª
curl -X PUT http://localhost:8083/connectors/mysql-source-connector/pause

# Ø§Ø³ØªØ¦Ù†Ø§Ù
curl -X PUT http://localhost:8083/connectors/mysql-source-connector/resume

# Ø­Ø§Ù„Ø© Connector
curl http://localhost:8083/connectors/mysql-source-connector/status
```

---

## Ø¥Ø¯Ø§Ø±Ø© Topics

### CLI Commands

```bash
# Ø¥Ù†Ø´Ø§Ø¡ Topic
kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --partitions 3 \
  --replication-factor 2

# Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Topics
kafka-topics.sh --list \
  --bootstrap-server localhost:9092

# ÙˆØµÙ Topic
kafka-topics.sh --describe \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# Ø­Ø°Ù Topic
kafka-topics.sh --delete \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Partitions
kafka-topics.sh --alter \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --partitions 5

# ØªØºÙŠÙŠØ± Configurations
kafka-configs.sh --alter \
  --bootstrap-server localhost:9092 \
  --entity-type topics \
  --entity-name my-topic \
  --add-config retention.ms=86400000
```

### Producer Ù…Ù† CLI

```bash
# Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ø¦Ù„
kafka-console-producer.sh \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# Ù…Ø¹ key
kafka-console-producer.sh \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --property "parse.key=true" \
  --property "key.separator=:"
```

### Consumer Ù…Ù† CLI

```bash
# Ù‚Ø±Ø§Ø¡Ø© Ø±Ø³Ø§Ø¦Ù„
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic my-topic

# Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --from-beginning

# Ù…Ø¹ key
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --property print.key=true \
  --property key.separator=":"

# Ù…Ø¹ consumer group
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic my-topic \
  --group my-group
```

### Consumer Groups Management

```bash
# Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Consumer Groups
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --list

# ÙˆØµÙ Consumer Group
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group my-group \
  --describe

# Reset Offsets
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group my-group \
  --reset-offsets \
  --to-earliest \
  --topic my-topic \
  --execute

# Ø­Ø°Ù Consumer Group
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group my-group \
  --delete
```

---

## Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ©

### Ù…Ø«Ø§Ù„ 1: Real-time Order Processing System

```java
// Order POJO
class Order {
    private String orderId;
    private String customerId;
    private String product;
    private int quantity;
    private double price;
    private String status;

    // Constructor, Getters, Setters
}

// Order Producer
public class OrderProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", JsonSerializer.class.getName());

        KafkaProducer<String, Order> producer = new KafkaProducer<>(props);

        // Ù…Ø­Ø§ÙƒØ§Ø© Ø·Ù„Ø¨Ø§Øª
        for (int i = 1; i <= 100; i++) {
            Order order = new Order(
                "ORD-" + i,
                "CUST-" + (i % 10),
                "Product-" + (i % 5),
                (int) (Math.random() * 10 + 1),
                Math.random() * 1000,
                "PENDING"
            );

            ProducerRecord<String, Order> record =
                new ProducerRecord<>("orders", order.getOrderId(), order);

            producer.send(record, (metadata, exception) -> {
                if (exception == null) {
                    System.out.println("Order sent: " + order.getOrderId() +
                        " to partition " + metadata.partition());
                }
            });

            try {
                Thread.sleep(100);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        producer.close();
    }
}

// Order Processor (Consumer)
public class OrderProcessor {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "order-processors");
        props.put("key.deserializer", StringDeserializer.class.getName());
        props.put("value.deserializer", JsonDeserializer.class.getName());
        props.put("enable.auto.commit", "false");

        KafkaConsumer<String, Order> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Arrays.asList("orders"));

        KafkaProducer<String, Order> producer = createProducer();

        try {
            while (true) {
                ConsumerRecords<String, Order> records =
                    consumer.poll(Duration.ofMillis(100));

                for (ConsumerRecord<String, Order> record : records) {
                    Order order = record.value();

                    // Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨
                    System.out.println("Processing order: " + order.getOrderId());

                    // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©
                    order.setStatus("PROCESSING");

                    // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø®Ø²ÙˆÙ†
                    if (checkInventory(order)) {
                        order.setStatus("CONFIRMED");
                        // Ø¥Ø±Ø³Ø§Ù„ Ù„Ù€ topic Ø¢Ø®Ø±
                        producer.send(new ProducerRecord<>(
                            "confirmed-orders", order.getOrderId(), order));
                    } else {
                        order.setStatus("OUT_OF_STOCK");
                        producer.send(new ProducerRecord<>(
                            "failed-orders", order.getOrderId(), order));
                    }
                }

                consumer.commitSync();
            }
        } finally {
            consumer.close();
            producer.close();
        }
    }

    private static KafkaProducer<String, Order> createProducer() {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", JsonSerializer.class.getName());
        return new KafkaProducer<>(props);
    }

    private static boolean checkInventory(Order order) {
        // Ù…Ø­Ø§ÙƒØ§Ø© ÙØ­Øµ Ø§Ù„Ù…Ø®Ø²ÙˆÙ†
        return Math.random() > 0.2; // 80% Ù†Ø¬Ø§Ø­
    }
}
```

### Ù…Ø«Ø§Ù„ 2: Log Aggregation System

```java
// Log Entry
class LogEntry {
    private String timestamp;
    private String level;
    private String service;
    private String message;

    // Constructor, Getters, Setters
}

// Log Producer (Ù…Ù† Ø®Ø¯Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©)
public class ServiceLogProducer {
    private static final KafkaProducer<String, LogEntry> producer;

    static {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", JsonSerializer.class.getName());
        producer = new KafkaProducer<>(props);
    }

    public static void log(String service, String level, String message) {
        LogEntry entry = new LogEntry(
            LocalDateTime.now().toString(),
            level,
            service,
            message
        );

        ProducerRecord<String, LogEntry> record =
            new ProducerRecord<>("application-logs", service, entry);

        producer.send(record);
    }

    public static void main(String[] args) {
        // Ù…Ø­Ø§ÙƒØ§Ø© logs Ù…Ù† Ø®Ø¯Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©
        String[] services = {"auth-service", "payment-service", "order-service"};
        String[] levels = {"INFO", "WARNING", "ERROR"};

        for (int i = 0; i < 1000; i++) {
            String service = services[(int) (Math.random() * services.length)];
            String level = levels[(int) (Math.random() * levels.length)];

            log(service, level, "Log message " + i);

            try {
                Thread.sleep(50);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        producer.close();
    }
}

// Log Analyzer (Kafka Streams)
public class LogAnalyzer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "log-analyzer");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");

        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, LogEntry> logs = builder.stream("application-logs");

        // 1. Ø¹Ø¯ Ø§Ù„Ù€ logs Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªÙˆÙ‰
        KTable<String, Long> logCounts = logs
            .groupBy((key, value) -> value.getLevel())
            .count();

        logCounts.toStream().to("log-counts");

        // 2. ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙ‚Ø·
        logs.filter((key, value) -> value.getLevel().equals("ERROR"))
            .to("error-logs");

        // 3. Ø¹Ø¯ Ø§Ù„Ù€ logs Ø­Ø³Ø¨ Ø§Ù„Ø®Ø¯Ù…Ø©
        KTable<String, Long> serviceCounts = logs
            .groupBy((key, value) -> value.getService())
            .count();

        serviceCounts.toStream().to("service-log-counts");

        // 4. Ù†Ø§ÙØ°Ø© Ø²Ù…Ù†ÙŠØ© - Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚Ø©
        KTable<Windowed<String>, Long> errorCountsPerMinute = logs
            .filter((key, value) -> value.getLevel().equals("ERROR"))
            .groupBy((key, value) -> value.getService())
            .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
            .count();

        errorCountsPerMinute.toStream()
            .map((key, value) -> KeyValue.pair(
                key.key() + "@" + key.window().start(),
                value
            ))
            .to("error-counts-per-minute");

        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();

        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

### Ù…Ø«Ø§Ù„ 3: Real-time Analytics Dashboard

```java
// User Event
class UserEvent {
    private String userId;
    private String eventType;  // CLICK, VIEW, PURCHASE
    private String page;
    private long timestamp;

    // Constructor, Getters, Setters
}

// Event Producer
public class UserEventProducer {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", JsonSerializer.class.getName());

        KafkaProducer<String, UserEvent> producer = new KafkaProducer<>(props);

        String[] eventTypes = {"CLICK", "VIEW", "PURCHASE"};
        String[] pages = {"home", "products", "cart", "checkout"};

        for (int i = 0; i < 10000; i++) {
            UserEvent event = new UserEvent(
                "user-" + (int) (Math.random() * 100),
                eventTypes[(int) (Math.random() * eventTypes.length)],
                pages[(int) (Math.random() * pages.length)],
                System.currentTimeMillis()
            );

            producer.send(new ProducerRecord<>(
                "user-events", event.getUserId(), event));

            try {
                Thread.sleep(10);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        producer.close();
    }
}

// Analytics Processor
public class AnalyticsProcessor {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "analytics-app");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");

        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, UserEvent> events = builder.stream("user-events");

        // 1. Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        events.groupBy((key, value) -> value.getEventType())
            .count()
            .toStream()
            .to("event-type-counts");

        // 2. Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ø­Ø³Ø¨ Ø§Ù„ØµÙØ­Ø©
        events.groupBy((key, value) -> value.getPage())
            .count()
            .toStream()
            .to("page-view-counts");

        // 3. Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§Øª Ù„ÙƒÙ„ Ù…Ø³ØªØ®Ø¯Ù…
        events.filter((key, value) -> value.getEventType().equals("PURCHASE"))
            .groupByKey()
            .count()
            .toStream()
            .to("user-purchase-counts");

        // 4. Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„ (Conversion Rate)
        KTable<String, Long> views = events
            .filter((key, value) -> value.getEventType().equals("VIEW"))
            .groupBy((key, value) -> value.getPage())
            .count();

        KTable<String, Long> purchases = events
            .filter((key, value) -> value.getEventType().equals("PURCHASE"))
            .groupBy((key, value) -> value.getPage())
            .count();

        // Join views and purchases
        views.join(purchases,
            (viewCount, purchaseCount) -> {
                double conversionRate = (double) purchaseCount / viewCount * 100;
                return String.format("%.2f%%", conversionRate);
            })
            .toStream()
            .to("conversion-rates");

        // 5. Ù†Ø§ÙØ°Ø© Ø²Ù…Ù†ÙŠØ© - Ø§Ù„Ø£Ø­Ø¯Ø§Ø« ÙƒÙ„ 30 Ø«Ø§Ù†ÙŠØ©
        events.groupBy((key, value) -> value.getEventType())
            .windowedBy(TimeWindows.of(Duration.ofSeconds(30)))
            .count()
            .toStream()
            .map((key, value) -> KeyValue.pair(
                key.key() + "@" + key.window().start(),
                value
            ))
            .to("events-per-30-seconds");

        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();

        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

---

## Best Practices

### 1. Producer Best Practices

```java
// âœ… Ø§Ø³ØªØ®Ø¯Ù… idempotence
props.put("enable.idempotence", true);

// âœ… Ø§Ø³ØªØ®Ø¯Ù… compression
props.put("compression.type", "snappy");

// âœ… Ø§Ø³ØªØ®Ø¯Ù… batching
props.put("batch.size", 16384);
props.put("linger.ms", 10);

// âœ… handle errors properly
producer.send(record, (metadata, exception) -> {
    if (exception != null) {
        // log error and retry
        logger.error("Failed to send message", exception);
    }
});

// âœ… close producer properly
Runtime.getRuntime().addShutdownHook(new Thread(() -> {
    producer.close(Duration.ofSeconds(10));
}));
```

### 2. Consumer Best Practices

```java
// âœ… Ø§Ø³ØªØ®Ø¯Ù… consumer group
props.put("group.id", "my-consumer-group");

// âœ… handle rebalancing
consumer.subscribe(topics, new ConsumerRebalanceListener() {
    // ...
});

// âœ… commit offsets properly
consumer.commitAsync((offsets, exception) -> {
    if (exception != null) {
        consumer.commitSync();  // retry with sync
    }
});

// âœ… handle errors and retry
try {
    processRecord(record);
} catch (Exception e) {
    // send to DLQ (Dead Letter Queue)
    sendToDeadLetterQueue(record);
}
```

### 3. Topic Design

```bash
# âœ… Ø¹Ø¯Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ù…Ù† Partitions
# Rule of thumb: throughput / partition throughput
# Ù…Ø«Ø§Ù„: 1GB/s Ã· 50MB/s = 20 partitions

# âœ… Replication factor >= 2
--replication-factor 3

# âœ… Retention policy
--config retention.ms=604800000  # 7 days
--config retention.bytes=1073741824  # 1GB

# âœ… Cleanup policy
--config cleanup.policy=delete  # or compact
```

### 4. Performance Tuning

```java
// Producer
props.put("acks", "1");  // balance between speed and reliability
props.put("compression.type", "lz4");  // Ø£Ø³Ø±Ø¹ Ù…Ù† gzip
props.put("batch.size", 32768);
props.put("buffer.memory", 67108864);  // 64MB

// Consumer
props.put("fetch.min.bytes", "1024");
props.put("fetch.max.wait.ms", "500");
props.put("max.poll.records", "500");
```

---

## Monitoring Ùˆ Troubleshooting

### Metrics Ø§Ù„Ù…Ù‡Ù…Ø©

```java
// Producer Metrics
- record-send-rate
- record-error-rate
- request-latency-avg
- batch-size-avg
- compression-rate-avg

// Consumer Metrics
- records-consumed-rate
- fetch-latency-avg
- records-lag-max
- commit-latency-avg

// Broker Metrics
- MessagesInPerSec
- BytesInPerSec
- BytesOutPerSec
- UnderReplicatedPartitions
- ISRShrinkRate
```

### Common Issues

```bash
# 1. Consumer Lag
# Ø§Ù„Ø­Ù„: Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ consumers Ø£Ùˆ partitions

# 2. Rebalancing ÙƒØ«ÙŠØ±
# Ø§Ù„Ø­Ù„: Ø²ÙŠØ§Ø¯Ø© session.timeout.ms Ùˆ max.poll.interval.ms

# 3. Out of Memory
# Ø§Ù„Ø­Ù„: ØªÙ‚Ù„ÙŠÙ„ batch.size Ùˆ buffer.memory

# 4. Data Loss
# Ø§Ù„Ø­Ù„: Ø§Ø³ØªØ®Ø¯Ø§Ù… acks=all Ùˆ min.insync.replicas=2
```

---

## Ø§Ù„Ø®Ù„Ø§ØµØ©

### Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… KafkaØŸ

âœ… **Real-time data streaming** - Ø¨Ø« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ©
âœ… **Event-driven architecture** - Ø¨Ù†ÙŠØ© Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø¯Ø§Ø«
âœ… **Log aggregation** - ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
âœ… **Microservices communication** - Ø§Ù„ØªÙˆØ§ØµÙ„ Ø¨ÙŠÙ† Ø§Ù„Ø®Ø¯Ù…Ø§Øª
âœ… **Activity tracking** - ØªØªØ¨Ø¹ Ø§Ù„Ù†Ø´Ø§Ø·
âœ… **Metrics collection** - Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³

### Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

ğŸ¯ **Topic** - ÙØ¦Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
ğŸ¯ **Partition** - ØªÙ‚Ø³ÙŠÙ… Ù„Ù„ØªÙˆØ§Ø²ÙŠ
ğŸ¯ **Offset** - Ø±Ù‚Ù… ØªØ³Ù„Ø³Ù„ÙŠ
ğŸ¯ **Producer** - ÙŠØ±Ø³Ù„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
ğŸ¯ **Consumer** - ÙŠÙ‚Ø±Ø£ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
ğŸ¯ **Consumer Group** - Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø³ØªÙ‡Ù„ÙƒÙŠÙ†
ğŸ¯ **Broker** - Ø®Ø§Ø¯Ù… Kafka

### Ø§Ù„Ø£Ø¯ÙˆØ§Øª

ğŸ”§ **Producer API** - Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
ğŸ”§ **Consumer API** - Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
ğŸ”§ **Streams API** - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
ğŸ”§ **Connect API** - Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø£Ù†Ø¸Ù…Ø© Ø®Ø§Ø±Ø¬ÙŠØ©

---

**ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙ…Ø±Ø¬Ø¹ Ø´Ø§Ù…Ù„ Ù„Ù€ Apache Kafka**
